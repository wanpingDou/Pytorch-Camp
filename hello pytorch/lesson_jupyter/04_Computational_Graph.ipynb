{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wanpingDou/Pytorch-Camp/blob/master/hello%20pytorch/lesson_jupyter/04_Computational_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWRzoBOi8Gnc"
   },
   "source": [
    "# 计算图\n",
    "\n",
    "pytorch最大的特性——动态图机制，动态图机制是pytorch与tensorflow最大的区别，该部分首先介绍计算图的概念，并通过演示动态图与静态图的搭建过程来理解动态图与静态图的差异。\n",
    "\n",
    "\n",
    "### 计算图的两大要素\n",
    "\n",
    "![](./img/CG_1.png)\n",
    "\n",
    "### 计算图梯度求导\n",
    "\n",
    "![](./img/CG_2.png)![](./img/CG_3.png)\n",
    "\n",
    "### 重要函数\n",
    "\n",
    "\n",
    "![](./img/CG_4.png)\n",
    "\n",
    "- Tensor.retain_grad：显式地保存非叶节点的梯度，代价就是会增加显存的消耗。用于判断该tensor是否需要被跟踪，用以计算梯度，默认为False。\n",
    "- Tensor.hook：在反向计算时直接打印，不会增加显存消耗，但是使用起来retain_grad()要比hook函数方便一些。\n",
    "- data：存放的是该张量的数据；\n",
    "- grad：初始为None；requires_grad = False时为None，否则当某out节点调用out.backward()时，生成新tensor节点，存放计算后的∂out/∂x梯度值，梯度值不会自动清空(下次调用out.backward()时可累积)\n",
    "- grad_fn：反向传播时，用来计算梯度的函数；\n",
    "- is_leaf：表明该tensor是否为叶子节点\n",
    "\n",
    "### [静态计算图vs动态计算图](https://zhuanlan.zhihu.com/p/145353262)\n",
    "\n",
    "![](./img/CG_5.png)![](./img/CG_6.png)\n",
    "\n",
    "- `静态计算图`\n",
    "\n",
    "> 理论上神经网络模型定义好以后就无需更改，当计算图构建好以后，在一轮轮的前向传播/反向传播迭代中可以重复使用此计算图，只不过将计算的梯度值不断更新到每个变量节点处即可，这种方式称为静态计算图，而早期的tensorflow采用的就是静态计算图的方式。\n",
    "\n",
    "- `动态计算图`\n",
    "\n",
    "> 什么是动态计算图 ？和静态图相反，pytorch在设计中采取了动态计算图的方式，即反向传播的计算图是动态更新的。每一轮反向传播开始时(前向传播结束后)都会动态的重新构建一个计算图，当本次反向传播完成后，计算图再次销毁。这种动态更新的方式允许用户在迭代过程中更改网络的形状和大小。\n",
    "\n",
    "- `个人理解`\n",
    "\n",
    "> 理论上静态图，更节省内存，速度更快；动态图更灵活自由，不过牺牲了部分速度和内存空间。但实际并不一定，因为深度学习框架不仅仅是计算图，还涉及到其他代码和底层优化，所以究竟是哪种更好，需要自行判断～\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "qeKyAR5s8E2P",
    "outputId": "bc83411f-1183-4307-db82-b0db2b57e586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "is_leaf:\n",
      " True True False False False\n",
      "gradient:\n",
      " tensor([5.]) tensor([2.]) tensor([2.]) None None\n",
      "grad_fn:\n",
      " None None <AddBackward0 object at 0x7fa3fc788f28> <AddBackward0 object at 0x7fa3f4624b70> <MulBackward0 object at 0x7fa3f4624b38>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "@file name  : lesson-04-Computational-Graph.py\n",
    "@author     : TingsongYu https://github.com/TingsongYu\n",
    "@date       : 2018-08-28\n",
    "@brief      : 计算图示例\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "# a.retain_grad()  # 记录a的梯度\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# 查看叶子结点\n",
    "print(\"is_leaf:\\n\", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "\n",
    "# 查看梯度\n",
    "print(\"gradient:\\n\", w.grad, x.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    "# 查看 grad_fn\n",
    "print(\"grad_fn:\\n\", w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOncuTcndCCN0MfZ1Xvk/6z",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1s81X4pIkCWnnbJFjxwrm5Hr-siVugJqe",
   "name": "04.Computational-Graph.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
