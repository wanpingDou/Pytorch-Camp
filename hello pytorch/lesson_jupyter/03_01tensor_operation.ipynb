{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03.tensor_operation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YdtFJluD4lgd4SCQbcSpBzIZAec0EKN4",
      "authorship_tag": "ABX9TyM9ifMA8SNmrrooScGs3GfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanpingDou/Pytorch-Camp/blob/master/hello%20pytorch/lesson_jupyter/03_tensor_operation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz-l4Ed_ZVKQ",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch中张量的操作：拼接、切分、索引、变换和运算\n",
        "\n",
        "---\n",
        "其他：\n",
        "\n",
        "- torch.t：变换\n",
        "> 功能：2维张量转置，对矩阵而言，等价于torch.transpose(input, 0, 1)\n",
        "\n",
        "- torch.manual_seed(1)：[利用随机数种子来使pytorch中的结果可以复现](https://cloud.tencent.com/developer/article/1149041)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep3XMByqZSME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "\"\"\"\n",
        "@file name  : lesson-03.py\n",
        "@author     : TingsongYu https://github.com/TingsongYu\n",
        "@date       : 2018-08-26\n",
        "@brief      : 张量操作\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(1)\n",
        "flag = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzhJsA4hwByn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1. 拼接：cat\n",
        "\n",
        "> - torch.cat：拼接\n",
        "  - dim：按照某一维度拼接\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEyMKov_W_W4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "e1fa1573-eb62-4eb6-8ee3-99ac1b0f2134"
      },
      "source": [
        "# ======================================= example 1 =======================================\n",
        "# torch.cat\n",
        "\n",
        "if flag:\n",
        "    t = torch.ones((2, 3))\n",
        "\n",
        "    t_0 = torch.cat([t, t], dim=0)\n",
        "    t_1 = torch.cat([t, t, t], dim=1)\n",
        "    \n",
        "    print(\"t:\\n{} shape:{}\\n\\nt_0:\\n{} shape:{}\\n\\nt_1:\\n{} shape:{}\".format(t, t.shape, \n",
        "                                          t_0, t_0.shape, \n",
        "                                          t_1, t_1.shape))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) shape:torch.Size([2, 3])\n",
            "\n",
            "t_0:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) shape:torch.Size([4, 3])\n",
            "\n",
            "t_1:\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]]) shape:torch.Size([2, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omPsQmLHwA91",
        "colab_type": "text"
      },
      "source": [
        "## 2. 拼接：stack\n",
        "> - [pytorch函数学习随笔记录---stack与cat](https://zhuanlan.zhihu.com/p/70035580)\n",
        "  - torch.cat会按照dim的维度进行拼接\n",
        "  - torch.stack会按照dim新增一维，然后跟cat一样拼接"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHf0sJnhXG7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "41984357-473f-404f-9c3b-eec2d7ccfc68"
      },
      "source": [
        "# ======================================= example 2 =======================================\n",
        "# torch.stack\n",
        "\n",
        "if flag:\n",
        "    t = torch.ones((2, 3))\n",
        "\n",
        "    t_stack0 = torch.stack([t, t], dim=0)\n",
        "\n",
        "    t_stack1 = torch.stack([t, t], dim=1)\n",
        "\n",
        "    print(\"\\nt:\\n{} shape:{}\\n\\nt_stack0:\\n{} shape:{}\\n\\nt_stack1:\\n{} shape:{}\".format(t, t.shape,\n",
        "                                                t_stack0, t_stack0.shape,\n",
        "                                                t_stack1, t_stack1.shape))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "t:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) shape:torch.Size([2, 3])\n",
            "\n",
            "t_stack0:\n",
            "tensor([[[1., 1., 1.],\n",
            "         [1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1.],\n",
            "         [1., 1., 1.]]]) shape:torch.Size([2, 2, 3])\n",
            "\n",
            "t_stack1:\n",
            "tensor([[[1., 1., 1.],\n",
            "         [1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1.],\n",
            "         [1., 1., 1.]]]) shape:torch.Size([2, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Rcvjt9wAYF",
        "colab_type": "text"
      },
      "source": [
        "## 3. 切分：chunk\n",
        "- torch.chunk：切分\n",
        "> 功能：`将张量按维度dim进行平均切分`\n",
        ">\n",
        "> 返回值：`张量列表`\n",
        "  - input：要切分的张量\n",
        "  - chunks：要切分的份数\n",
        "  - dim：要切分的维度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVzRM-fFXG-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "61fdee59-f863-45c2-a516-b07cf4c2d104"
      },
      "source": [
        "# ======================================= example 3 =======================================\n",
        "# torch.chunk\n",
        "\n",
        "if flag:\n",
        "    a = torch.ones((2, 7))  # 7\n",
        "    list_of_tensors = torch.chunk(a, dim=1, chunks=3)   # 3\n",
        "\n",
        "    for idx, t in enumerate(list_of_tensors):\n",
        "        print(\"第{}个张量：\\n{}, shape is {}\\n\".format(idx+1, t, t.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第1个张量：\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
            "\n",
            "第2个张量：\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
            "\n",
            "第3个张量：\n",
            "tensor([[1.],\n",
            "        [1.]]), shape is torch.Size([2, 1])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNnfcGpGv_XO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 4. 切分：split\n",
        "- torch.split：切分\n",
        "> 功能：`将张量按维度dim进行切分`\n",
        ">\n",
        "> 返回值：`张量列表`\n",
        "  - split_size_or_sections：为int时，表示每一份的长度；为list时，按list元素切分\n",
        "  - dim : 要切分的维度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nncv4aHXHY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "fafb3215-c043-49bf-c90b-61d63bffc11e"
      },
      "source": [
        "# ======================================= example 4 =======================================\n",
        "# torch.split\n",
        "\n",
        "if flag: \n",
        "    print('==================== split_size_or_sections为整数时：==================== ')\n",
        "    t = torch.ones((2, 5))\n",
        "    list_of_tensors = torch.split(t, 2, dim=1)  # [2 , 1, 2]\n",
        "    for idx, t in enumerate(list_of_tensors):\n",
        "        print(\"第{}个张量：\\n{}, shape is {}\\n\".format(idx, t, t.shape))\n",
        "\n",
        "    print('==================== split_size_or_sections为list时：==================== ')\n",
        "    t = torch.ones((2, 5))\n",
        "    list_of_tensors = torch.split(t, [2, 1, 2], dim=1)\n",
        "    for idx, t in enumerate(list_of_tensors):\n",
        "        print(\"第{}个张量：\\n{}, shape is {}\\n\".format(idx, t, t.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== split_size_or_sections为整数时：==================== \n",
            "第0个张量：\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]]), shape is torch.Size([2, 2])\n",
            "\n",
            "第1个张量：\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]]), shape is torch.Size([2, 2])\n",
            "\n",
            "第2个张量：\n",
            "tensor([[1.],\n",
            "        [1.]]), shape is torch.Size([2, 1])\n",
            "\n",
            "==================== split_size_or_sections为list时：==================== \n",
            "第0个张量：\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]]), shape is torch.Size([2, 2])\n",
            "\n",
            "第1个张量：\n",
            "tensor([[1.],\n",
            "        [1.]]), shape is torch.Size([2, 1])\n",
            "\n",
            "第2个张量：\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]]), shape is torch.Size([2, 2])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwXJK39lv-1W",
        "colab_type": "text"
      },
      "source": [
        "## 5. 索引：index_select\n",
        "- torch.index_select：索引\n",
        "> 功能：`在维度dim上，按index索引数据`\n",
        ">\n",
        "> 返回值：`索引得到的数据拼接的张量`\n",
        "  - input：要索引的张量\n",
        "  - dim：要索引的维度\n",
        "  - index：要索引数据的序号，注意dtype是torch.long"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTT23YcaXhJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "30900e45-6187-410e-f092-8b03291f2a6b"
      },
      "source": [
        "# ======================================= example 5 =======================================\n",
        "# torch.index_select\n",
        "\n",
        "if flag:\n",
        "    t = torch.randint(0, 9, size=(3, 3))\n",
        "    idx = torch.tensor([0, 2], dtype=torch.long)    # float\n",
        "    t_select = torch.index_select(t, dim=0, index=idx)\n",
        "    print(\"t:\\n{}\\nt_select:\\n{}\".format(t, t_select))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t:\n",
            "tensor([[4, 5, 0],\n",
            "        [5, 7, 1],\n",
            "        [2, 5, 8]])\n",
            "t_select:\n",
            "tensor([[4, 5, 0],\n",
            "        [2, 5, 8]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J9mGsrrv-hm",
        "colab_type": "text"
      },
      "source": [
        "## 6. 索引：masked_select\n",
        "- torch.masked_select：索引\n",
        "> 功能：`按mask中的True进行索引`\n",
        "> \n",
        "> 返回值：`一维张量`\n",
        "  - input：要索引的张量\n",
        "  - mask：与input同形状的布尔类型张量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At9tRFedXhMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "806adfd1-ba64-4b2d-9aca-af481e561061"
      },
      "source": [
        "# ======================================= example 6 =======================================\n",
        "# torch.masked_select\n",
        "\n",
        "if flag:\n",
        "    t = torch.randint(0, 9, size=(3, 3))\n",
        "    mask = t.le(5)  # ge is mean greater than or equal/   gt: greater than  le  lt\n",
        "    t_select = torch.masked_select(t, mask)\n",
        "    print(\"t:\\n{}\\nmask:\\n{}\\nt_select:\\n{} \".format(t, mask, t_select))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t:\n",
            "tensor([[0, 2, 3],\n",
            "        [1, 8, 4],\n",
            "        [0, 3, 6]])\n",
            "mask:\n",
            "tensor([[ True,  True,  True],\n",
            "        [ True, False,  True],\n",
            "        [ True,  True, False]])\n",
            "t_select:\n",
            "tensor([0, 2, 3, 1, 4, 0, 3]) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOiAcGGJv-A2",
        "colab_type": "text"
      },
      "source": [
        "## 7. 变换：reshape\n",
        "- torch.reshape：变换\n",
        "> 功能：`改变张量形状`\n",
        "> \n",
        "> 注意事项：当张量在内存中是连续的时，新张量与input共享数据内存。这种共享与out不同，out是整个tensor都共享内存，相当于别名；reshape是仅data共享内存。改变一个张量的数据，另一个张量会跟着改变。\n",
        "  - input：要变换的张量\n",
        "  - shape：新张量的形状",
        "\n",
        ".data和.detach函数备注：.data 和.detach只取出本体tensor数据，舍弃了grad，grad_fn等额外反向图计算过程需保存的额外信息。",
        "- .data取出本体tensor后仍与原数据共享内存，在使用in-place操作后，会修改原数据的值，而如果在反向传播过程中使用到原数据会导致计算错误。",
        "- .detach，如果在反向传播过程中发现原数据被修改过会报错，更加安全"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU3a-3prXmeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "78f5ee90-7db9-4495-b236-2facec68de6d"
      },
      "source": [
        "# ======================================= example 7 =======================================\n",
        "# torch.reshape\n",
        "\n",
        "if flag:\n",
        "    t = torch.randperm(8)  # torch.randperm(n, out=None):给定参数n，返回一个从0到n-1的随机整数排列 \n",
        "    t_reshape = torch.reshape(t, (-1, 2, 2))    # -1\n",
        "    print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
        "\n",
        "    t[0] = 1024\n",
        "    print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
        "    print(\"t.data 内存地址:{}\".format(id(t.data)))\n",
        "    print(\"t_reshape.data 内存地址:{}\".format(id(t_reshape.data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t:tensor([2, 0, 1, 6, 3, 4, 7, 5])\n",
            "t_reshape:\n",
            "tensor([[[2, 0],\n",
            "         [1, 6]],\n",
            "\n",
            "        [[3, 4],\n",
            "         [7, 5]]])\n",
            "t:tensor([1024,    0,    1,    6,    3,    4,    7,    5])\n",
            "t_reshape:\n",
            "tensor([[[1024,    0],\n",
            "         [   1,    6]],\n",
            "\n",
            "        [[   3,    4],\n",
            "         [   7,    5]]])\n",
            "t.data 内存地址:140384043357960\n",
            "t_reshape.data 内存地址:140384043357960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS3y5Pbrv9eO",
        "colab_type": "text"
      },
      "source": [
        "## 8. 变换：transpose\n",
        "- torch.transpose：变换\n",
        "> 功能：交换张量的两个维度。在图像的预处理中，有时读取的图像数据是(c, h, w)，但是我们常用的是(h, w, c)，就需要用此方法把channel和width交换，然后width和height交换\n",
        "  - input：要变换的张量\n",
        "  - dim0：要交换的维度\n",
        "  - dim1：要交换的维度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkiCnhf_XHc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cb8b5f44-7bcb-4771-a260-6e627c65c259"
      },
      "source": [
        "# ======================================= example 8 =======================================\n",
        "# torch.transpose\n",
        "\n",
        "if flag:\n",
        "    # torch.transpose\n",
        "    t = torch.rand((2, 3, 4))\n",
        "    t_transpose = torch.transpose(t, dim0=1, dim1=2)    # c*h*w     h*w*c\n",
        "    print(\"t shape:{}\\nt_transpose shape: {}\".format(t.shape, t_transpose.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t shape:torch.Size([2, 3, 4])\n",
            "t_transpose shape: torch.Size([2, 4, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBZAUYzev83-",
        "colab_type": "text"
      },
      "source": [
        "## 9. 压缩：squeeze\n",
        "- torch.squeeze：压缩\n",
        "> 功能：压缩长度为1的维度（轴）\n",
        "  - dim：若为None，移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4JenKCDXyhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "259b6489-0661-4208-ef30-ee2927310fdc"
      },
      "source": [
        "# ======================================= example 9 =======================================\n",
        "# torch.squeeze\n",
        "\n",
        "if flag:\n",
        "    t = torch.rand((1, 2, 3, 1))\n",
        "    t_sq = torch.squeeze(t)\n",
        "    t_0 = torch.squeeze(t, dim=0)\n",
        "    t_1 = torch.squeeze(t, dim=1)\n",
        "    print(t.shape)\n",
        "    print(t_sq.shape)\n",
        "    print(t_0.shape)\n",
        "    print(t_1.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 3, 1])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3, 1])\n",
            "torch.Size([1, 2, 3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPdd9Dxnv6JC",
        "colab_type": "text"
      },
      "source": [
        "## 10. 拓展：unsqueeze\n",
        "- torch.unsqueeze：拓展\n",
        "> 功能：依据dim扩展维度\n",
        "  - dim: 扩展的维度。必须指定，否则会报错。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rPYOYZuym1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a56a4026-7e3b-448e-ec85-d37141ddb777"
      },
      "source": [
        "# ======================================= example 10 =======================================\n",
        "# torch.squeeze\n",
        "\n",
        "if flag:\n",
        "    t = torch.rand((1, 2, 3, 1))\n",
        "    t_0 = torch.unsqueeze(t, dim=0)\n",
        "    t_1 = torch.unsqueeze(t, dim=1)\n",
        "    print(t.shape)\n",
        "    print(t_0.shape)\n",
        "    print(t_1.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 3, 1])\n",
            "torch.Size([1, 1, 2, 3, 1])\n",
            "torch.Size([1, 1, 2, 3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJDbayY1yoH5",
        "colab_type": "text"
      },
      "source": [
        "## 11. 运算\n",
        "\n",
        "- add(input, other, *, alpha=1, out=None)\n",
        "    \n",
        "$$ \\text{out} = \\text{input} + \\text{alpha} \\times \\text{other} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm1WcFlPW_iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "e0ff7eac-d53b-40f8-8a11-20cbe196710c"
      },
      "source": [
        "# ======================================= example 11 =======================================\n",
        "# torch.add\n",
        "\n",
        "if flag:\n",
        "    t_0 = torch.randn((3, 3))\n",
        "    t_1 = torch.ones_like(t_0)\n",
        "    t_add = torch.add(input=t_0, alpha=10, other=t_1)\n",
        "\n",
        "    print(\"t_0:\\n{}\\nt_1:\\n{}\\nt_add_10:\\n{}\".format(t_0, t_1, t_add))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_0:\n",
            "tensor([[-1.0273, -0.7286, -0.7209],\n",
            "        [ 1.1115,  0.1889,  0.3040],\n",
            "        [ 0.2830,  2.0379,  0.1633]])\n",
            "t_1:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "t_add_10:\n",
            "tensor([[ 8.9727,  9.2714,  9.2791],\n",
            "        [11.1115, 10.1889, 10.3040],\n",
            "        [10.2830, 12.0379, 10.1633]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
